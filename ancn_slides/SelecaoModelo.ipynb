{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escolha do modelo de regressão linear\n",
    "\n",
    "Vimos no início do bloco sobre **Ajuste de Curvas** que a **Interpolação Polinomial** pode não capturar bem a relação entre $x$ e $y$, embora sempre passe por todos os pontos dados.\n",
    "\n",
    "No exemplo que utilizamos, analisando o **Diagrama de Dispersão** era possível concluir que a **Regressão Linear Simples**, $f_1(x) = \\beta_0 + \\beta_1 x$ era muito mais adequada.\n",
    "\n",
    "Nem sempre é possível tirar conclusões deste tipo visualmente. Em certas situações, poderíamos nos perguntar se seria uma boa ideia adicionar um novo termo à regressão, resultando na função $f_2(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2$.\n",
    "\n",
    "Neste caso, nenhuma das métricas de qualidade de ajuste que vimos seria útil para decidir qual das duas funções é \"melhor\". O coeficiente de determinação, por exemplo, definido por\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\textrm{SQRes}}{\\textrm{SQTot}},\n",
    "$$\n",
    "\n",
    "onde $\\textrm{SQRes} = \\sum_i (y_i - f(x_i))^2$ e $\\textrm{SQTot} = \\sum_i (y_i - \\bar y)^2$, será menor para $f_2(x)$ que para $f_1(x)$, assumindo que os parâmetros sejam encontrados, em cada caso, pelo método dos quadrados mínimos.\n",
    "\n",
    "De fato, poderíamos continuar adicionando novos termos $\\beta_3 x^3, \\beta_4 x^4, \\ldots, \\beta_{n-1} x^{n-1}$ até que finalmente tivéssemos desvio $D(\\beta) = 0 \\Rightarrow R^2 = 1$. Neste caso, o resultado da regressão linear seria igual ao da interpolação polinomial. Claramente, o coeficiente de determinação $R^2$ não pode ser usado para decidir entre um modelo mais simples e outro com mais parâmetros.\n",
    "\n",
    "Na aula de hoje veremos duas maneiras de decidir qual é o melhor modelo dentre vários modelos possíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação cruzada\n",
    "\n",
    "Dentre um conjunto de modelos (funções) possíveis para relacionar preditores e a resposta, podemos considerar como sendo o **melhor** aquele que resulta no menor erro de predição para **pontos ainda não-observados**.\n",
    "\n",
    "Mas como podemos usar esta ideia se não temos como calcular o erro de predição para os pontos não-observados?\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "**Ideia:** Vamos \"fingir\" que não vimos alguns desses pontos ainda. Vamos separar X% dos pontos dados e usar o restante para encontar os parâmetros de cada um dos modelos (de regressão, neste caso) que queremos testar.\n",
    "\n",
    "**Q**: Mas como escolher estes pontos?\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "Mesmo que a escolha seja aleatória, poderíamos ter dado \"sorte\".\n",
    "\n",
    "**Q**: Como fazer para tornar este teste mais robusto?\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "**Ideia melhorada:** Vamos embaralhar o conjunto de pontos $(x_1,y_1), \\ldots, (x_n,y_n)$ e dividí-lo em $K$ partes de tamanho $n/K$. Denotemos cada uma dessas partes por $(X_k,Y_k)$, para $k=1,\\ldots,K$. Para cada modelo, vamos repetir o seguinte procedimento:\n",
    "\n",
    "\n",
    "* Erro = 0\n",
    "* Para $k=1,\\ldots,K$:\n",
    "    * Conjunto de teste: $(X_k,Y_k)$\n",
    "    * Conjunto de treinamento: $(X_1,Y_1), \\ldots, (X_{k-1},Y_{k-1}), (X_{k+1},Y_{k+1}), \\ldots, (X_{K},Y_{K})$.\n",
    "    * Obter os parâmetros $\\beta$ a partir dos pontos no conjunto de treinamento\n",
    "    * Para cada $(x,y)$ no conjunto de teste $(X_k,Y_k)$:\n",
    "        * Erro = Erro + $(y - f_\\beta (x))^2$\n",
    "* Erro = Erro / $n$\n",
    "\n",
    "Ao final, escolhemos o modelo que retorna o menor erro.\n",
    "\n",
    "**No exemplo a seguir**, vamos revisitar o problema da primeira aula e usar a validação cruzada para decidir entre $f_1(x) = \\beta_0 + \\beta_1 x$ e $f_2(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x119c7bd50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrfw/RW7+iBK2R4AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11994a050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "b = 2\n",
    "a = 1\n",
    "\n",
    "x = np.arange(20)\n",
    "y = a*x + b + (2*np.random.rand(len(x))-1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "plt.plot(x,y,'go')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  0,  4, 19, 14, 12,  3, 15,  7,  1,  8, 17, 10, 18, 13,  2, 16,\n",
       "        6,  9, 11])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embaralhando os pontos:\n",
    "inds = np.arange(20)\n",
    "np.random.shuffle(inds)\n",
    "shuffled_x = x[inds]\n",
    "shuffled_y = y[inds]\n",
    "shuffled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.293478686909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Validação cruzada para f(x) = beta0 + beta1x\n",
    "\n",
    "n = 20\n",
    "K = 4\n",
    "\n",
    "lr = LinearRegression()\n",
    "error = 0.0\n",
    "for k in range(K):\n",
    "    test_inds = np.array([False for i in range(n)])\n",
    "    test_inds[int(k*n/K):int((k+1)*n/K)] = True\n",
    "    \n",
    "    test_x = shuffled_x[test_inds].reshape(5,1)\n",
    "    test_y = shuffled_y[test_inds]\n",
    "    \n",
    "    train_x = shuffled_x[~test_inds].reshape(15,1)\n",
    "    train_y = shuffled_y[~test_inds]\n",
    "    \n",
    "    model_1 = lr.fit(X=train_x,y=train_y)\n",
    "    yhat = model_1.predict(test_x)\n",
    "    error += np.sum((test_y - yhat)**2)\n",
    "    \n",
    "error /= n\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.326513566633\n"
     ]
    }
   ],
   "source": [
    "# Validação cruzada para f(x) = beta0 + beta1 x + beta2 x^2\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "error = 0.0\n",
    "for k in range(K):\n",
    "    test_inds = np.array([False for i in range(n)])\n",
    "    test_inds[int(k*n/K):int((k+1)*n/K)] = True\n",
    "    \n",
    "    test_x = shuffled_x[test_inds]\n",
    "    test_x = np.array([test_x,test_x**2]).T\n",
    "    test_y = shuffled_y[test_inds]\n",
    "    \n",
    "    train_x = shuffled_x[~test_inds]\n",
    "    train_x = np.array([train_x,train_x**2]).T\n",
    "    train_y = shuffled_y[~test_inds]\n",
    "    \n",
    "    model_2 = lr.fit(X=train_x,y=train_y)\n",
    "    yhat = model_2.predict(test_x)\n",
    "    error += np.sum((test_y - yhat)**2)\n",
    "    \n",
    "error /= n\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularização\n",
    "\n",
    "A validação cruzada é uma excelente solução para comparar um pequeno número de modelos. Contudo, ao considerarmos todos os modelos possíveis dados $p$ preditores, teremos de ajustar e testar $2^p$ modelos. Para $p$ grande, isso impossibilita a seleção de modelos através da validação cruzada.\n",
    "\n",
    "Veremos agora uma maneira de escolher um modelo de regressão sem que seja preciso testar o conjunto potência dos preditores.\n",
    "\n",
    "Para motivar o método em questão, vamos voltar ao exemplo anterior. Lembre-se de que a regressão polinomial de grau $n-1$ a partir de $n$ pontos é idêntica a interpolação polinomial desses pontos. O excesso de preditores na regressão linear costuma resultar em valores altos para os parâmetros. Isto pode ser verificado a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabricio/anaconda2/envs/py3.5/lib/python3.5/site-packages/ipykernel_launcher.py:1: RankWarning: Polyfit may be poorly conditioned\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6.64389193e-14, -1.02135899e-11,  6.95630281e-10, -2.70936649e-08,\n",
       "        6.32291611e-07, -7.37798133e-06, -4.10546175e-05,  3.55465897e-03,\n",
       "       -7.79399714e-02,  1.06593890e+00, -1.03076380e+01,  7.32114987e+01,\n",
       "       -3.85444010e+02,  1.49368139e+03, -4.17220058e+03,  8.08587261e+03,\n",
       "       -1.01805356e+04,  7.36051437e+03, -2.26402367e+03,  1.37639006e+00])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef = np.polyfit(x,y,deg=19)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solução conhecida como **regularização** consiste em considerar **todos os preditores simultaneamente** e **penalizar** modelos com coeficientes (valores de parâmetros) muito elevados. Veremos a seguir dois tipos de regularização.\n",
    "\n",
    "### Regularização $L_1$ (LASSO)\n",
    "\n",
    "Consiste em impor uma restrição à soma dos módulos dos parâmetros $\\beta$ (exceto o intercepto $\\beta_0$):\n",
    "\n",
    "$$\n",
    "\\min_\\beta D(\\beta) = \\sum_{i=1}^n (y_i - f_\\beta(x_i))^2\\\\\n",
    "\\textrm{sujeito à $\\sum_{j=1}^p |\\beta_j| < s$}\n",
    "$$\n",
    "\n",
    "para alguma escolha de $s > 0$. Pode-se mostrar que isto é equivalente a\n",
    "\n",
    "$$\n",
    "\\min_\\beta \\sum_{i=1}^n (y_i - f_\\beta(x_i))^2 + \\alpha \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "\n",
    "para algum $\\alpha(s) > 0$.\n",
    "\n",
    "O efeito do termo de \"penalização\" $\\alpha \\sum_{j=1}^p |\\beta_j|$ é forçar que os $\\beta$'s sejam menores. Note que não estamos mais otimizando o desvio $D(\\beta)$. Ou seja, os parâmetros encontrados não serão os parâmetros de quadrados mínimos.\n",
    "\n",
    "Com a presença do módulo no termo de penalização, esta função objetivo deixa de ser derivável. Felizmente, uma solução aproximada pode ser encontrada em tempo polinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x*1.0\n",
    "x_powers = np.array([np.power(x,i) for i in range(1,19)]).T\n",
    "#x_powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1214901542478906 [ 9.71850356e-01  3.54654299e-03 -3.10308690e-04  1.24805949e-05\n",
      "  8.49748252e-07  2.50299909e-08  1.39669694e-10 -4.19188245e-11\n",
      " -3.79674990e-12 -2.31130423e-13 -1.15256639e-14 -4.85706029e-16\n",
      " -1.63063259e-17 -2.93979028e-19  1.52643412e-20  2.36499079e-21\n",
      "  1.99991736e-22  1.40399427e-23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabricio/anaconda2/envs/py3.5/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lr1 = Lasso(alpha=0.1)\n",
    "lr1.fit(x_powers,y)\n",
    "print(lr1.intercept_,lr1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularização $L_2$ (Ridge)\n",
    "\n",
    "A regularização $L_2$ é semelhante à $L_1$, mas usa a norma-2 como base para a penalização:\n",
    "\n",
    "$$\n",
    "\\min_\\beta D(\\beta) = \\sum_{i=1}^n (y_i - f_\\beta(x_i))^2\\\\\n",
    "\\textrm{sujeito à $\\sum_{j=1}^p \\beta_j^2 < s$}\n",
    "$$\n",
    "\n",
    "para alguma escolha de $s > 0$. Pode-se mostrar que isto é equivalente a\n",
    "\n",
    "$$\n",
    "\\min_\\beta  \\sum_{i=1}^n (y_i - f_\\beta(x_i))^2 + \\alpha \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "para algum $\\alpha(s) > 0$.\n",
    "\n",
    "De novo, o efeito do termo de \"penalização\" $\\alpha \\sum_{j=1}^p \\beta_j^2$ é forçar que os $\\beta$'s sejam menores. Esta penalidade é maior que no LASSO para $|\\beta_j| > 1$, mas menores que naquele para $|\\beta_j| < 1$.\n",
    "\n",
    "Ao contrário do LASSO, a solução exata para o Ridge pode ser calculada em tempo polinomial. Esta solução é dada por\n",
    "\n",
    "$$\n",
    "\\beta = (X^\\top X + \\alpha \\mathbf{I})^{-1} X^\\top y.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.505986784549927 [ 1.36516031e+00 -4.83493572e-07  1.11678717e-04  7.91826901e-02\n",
      "  4.14803667e-01 -7.35158137e-01  5.06316510e-01 -1.99025341e-01\n",
      "  5.10051809e-02 -9.08752555e-03  1.16429368e-03 -1.08983126e-04\n",
      "  7.47148221e-06 -3.71220470e-07  1.30126700e-08 -3.05146107e-10\n",
      "  4.29533208e-12 -2.74382777e-14]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lr2 = Ridge(alpha=0.1)\n",
    "lr2.fit(x_powers,y)\n",
    "print(lr2.intercept_,lr2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material suplementar\n",
    "\n",
    "[Complete tutorial Ridge and Lasso Regression in Python](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
