{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escolha do modelo de regressão linear\n",
    "\n",
    "Vimos no início do bloco sobre **Ajuste de Curvas** que a **Interpolação Polinomial** pode não capturar bem a relação entre $x$ e $y$, embora sempre passe por todos os pontos dados.\n",
    "\n",
    "No exemplo que utilizamos, analisando o **Diagrama de Dispersão** era possível concluir que a **Regressão Linear Simples**, $f_1(x) = \\beta_0 + \\beta_1 x$ era muito mais adequada.\n",
    "\n",
    "Nem sempre é possível tirar conclusões deste tipo visualmente. Em certas situações, poderíamos nos perguntar se seria uma boa ideia adicionar um novo termo à regressão, resultando na função $f_2(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2$.\n",
    "\n",
    "Neste caso, nenhuma das métricas de qualidade de ajuste que vimos seria útil para decidir qual das duas funções é \"melhor\". O coeficiente de determinação, por exemplo, definido por\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\textrm{SQRes}}{\\textrm{SQTot}},\n",
    "$$\n",
    "\n",
    "onde $\\textrm{SQRes} = \\sum_i (y_i - f(x_i))^2$ e $\\textrm{SQTot} = \\sum_i (y_i - \\bar y)^2$, será menor para $f_2(x)$ que para $f_1(x)$, assumindo que os parâmetros sejam encontrados, em cada caso, pelo método dos quadrados mínimos.\n",
    "\n",
    "De fato, poderíamos continuar adicionando novos termos $\\beta_3 x^3, \\beta_4 x^4, \\ldots, \\beta_{n-1} x^{n-1}$ até que finalmente tivéssemos desvio $D(\\beta) = 0 \\Rightarrow R^2 = 1$. Neste caso, o resultado da regressão linear seria igual ao da interpolação polinomial. Claramente, o coeficiente de determinação $R^2$ não pode ser usado para decidir entre um modelo mais simples e outro com mais parâmetros.\n",
    "\n",
    "Na aula de hoje veremos duas maneiras de decidir qual é o melhor modelo dentre vários modelos possíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação cruzada\n",
    "\n",
    "Dentre um conjunto de modelos (funções) possíveis para relacionar preditores e a resposta, podemos considerar como sendo o **melhor** aquele que resulta no menor erro de predição para **pontos ainda não-observados**.\n",
    "\n",
    "Mas como podemos usar esta ideia se não temos como calcular o erro de predição para os pontos não-observados?\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "**Ideia:** Vamos \"fingir\" que não vimos alguns desses pontos ainda. Vamos separar X% dos pontos dados e usar o restante para encontar os parâmetros de cada um dos modelos (de regressão, neste caso) que queremos testar.\n",
    "\n",
    "**Q**: Mas como escolher estes pontos?\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "Mesmo que a escolha seja aleatória, poderíamos ter dado \"sorte\".\n",
    "\n",
    "**Q**: Como fazer para tornar este teste mais robusto?\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "**Ideia melhorada:** Vamos embaralhar o conjunto de pontos $(x_1,y_1), \\ldots, (x_n,y_n)$ e dividí-lo em $K$ partes de tamanho $n/K$. Denotemos cada uma dessas partes por $(X_k,Y_k)$, para $k=1,\\ldots,K$. Para cada modelo, vamos repetir o seguinte procedimento:\n",
    "\n",
    "\n",
    "* Erro = 0\n",
    "* Para $k=1,\\ldots,K$:\n",
    "    * Conjunto de teste: $(X_k,Y_k)$\n",
    "    * Conjunto de treinamento: $(X_1,Y_1), \\ldots, (X_{k-1},Y_{k-1}), (X_{k+1},Y_{k+1}), \\ldots, (X_{K},Y_{K})$.\n",
    "    * Obter os parâmetros $\\beta$ a partir dos pontos no conjunto de treinamento\n",
    "    * Para cada $(x,y)$ no conjunto de teste $(X_k,Y_k)$:\n",
    "        * Erro = Erro + $(y - f_\\beta (x))^2$\n",
    "* Erro = Erro / $n$\n",
    "\n",
    "Ao final, escolhemos o modelo que retorna o menor erro.\n",
    "\n",
    "**No exemplo a seguir**, vamos revisitar o problema da primeira aula e usar a validação cruzada para decidir entre $f_1(x) = \\beta_0 + \\beta_1 x$ e $f_2(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'y')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "b = 2\n",
    "a = 1\n",
    "\n",
    "x = np.arange(20)\n",
    "y = a*x + b + (2*np.random.rand(len(x))-1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "plt.plot(x,y,'go')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 17,  6, 15, 10,  1,  7,  9,  0, 19,  8, 16,  2, 11,  4,  3,  5,\n",
       "       13, 12, 18])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embaralhando os pontos:\n",
    "inds = np.arange(20)\n",
    "np.random.shuffle(inds)\n",
    "shuffled_x = x[inds]\n",
    "shuffled_y = y[inds]\n",
    "shuffled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4371138773790613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Validação cruzada para f(x) = beta0 + beta1x\n",
    "\n",
    "n = 20\n",
    "K = 4\n",
    "\n",
    "lr = LinearRegression()\n",
    "error = 0.0\n",
    "for k in range(K):\n",
    "    test_inds = np.array([False for i in range(n)])\n",
    "    test_inds[int(k*n/K):int((k+1)*n/K)] = True\n",
    "    \n",
    "    test_x = shuffled_x[test_inds].reshape(5,1)\n",
    "    test_y = shuffled_x[test_inds]\n",
    "    \n",
    "    train_x = shuffled_x[~test_inds].reshape(15,1)\n",
    "    train_y = shuffled_y[~test_inds]\n",
    "    \n",
    "    model_1 = lr.fit(X=train_x,y=train_y)\n",
    "    yhat = model_1.predict(test_x)\n",
    "    error = np.sum((test_y - yhat)**2)\n",
    "    \n",
    "error /= n\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4421232885537378\n"
     ]
    }
   ],
   "source": [
    "# Validação cruzada para f(x) = beta0 + beta1 x + beta2 x^2\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "error = 0.0\n",
    "for k in range(K):\n",
    "    test_inds = np.array([False for i in range(n)])\n",
    "    test_inds[int(k*n/K):int((k+1)*n/K)] = True\n",
    "    \n",
    "    test_x = shuffled_x[test_inds]\n",
    "    test_x = np.array([test_x,test_x**2]).T\n",
    "    test_y = shuffled_x[test_inds]\n",
    "    \n",
    "    train_x = shuffled_x[~test_inds]\n",
    "    train_x = np.array([train_x,train_x**2]).T\n",
    "    train_y = shuffled_y[~test_inds]\n",
    "    \n",
    "    model_1 = lr.fit(X=train_x,y=train_y)\n",
    "    yhat = model_1.predict(test_x)\n",
    "    error = np.sum((test_y - yhat)**2)\n",
    "    \n",
    "error /= n\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularização\n",
    "\n",
    "A validação cruzada é uma excelente solução para comparar um pequeno número de modelos. Contudo, ao considerarmos todos os modelos possíveis dados $p$ preditores, teremos de ajustar e testar $2^p$ modelos. Para $p$ grande, isso impossibilita a seleção de modelos através da validação cruzada.\n",
    "\n",
    "Veremos agora uma maneira de escolher um modelo de regressão sem que seja preciso testar o conjunto potência dos preditores.\n",
    "\n",
    "Para motivar o método em questão, vamos voltar ao exemplo anterior. Lembre-se de que a regressão polinomial de grau $n-1$ a partir de $n$ pontos é idêntica a interpolação polinomial desses pontos. O excesso de preditores na regressão linear costuma resultar em valores altos para os parâmetros. Isto pode ser verificado a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabricio/anaconda2/envs/py3.5/lib/python3.5/site-packages/ipykernel_launcher.py:1: RankWarning: Polyfit may be poorly conditioned\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6.64389193e-14, -1.02135899e-11,  6.95630281e-10, -2.70936649e-08,\n",
       "        6.32291611e-07, -7.37798133e-06, -4.10546175e-05,  3.55465897e-03,\n",
       "       -7.79399714e-02,  1.06593890e+00, -1.03076380e+01,  7.32114987e+01,\n",
       "       -3.85444010e+02,  1.49368139e+03, -4.17220058e+03,  8.08587261e+03,\n",
       "       -1.01805356e+04,  7.36051437e+03, -2.26402367e+03,  1.37639006e+00])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef = np.polyfit(x,y,deg=19)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solução conhecida como **regularização** consiste em considerar **todos os preditores simultaneamente** e **penalizar** modelos com coeficientes (valores de parâmetros) muito elevados. Veremos a seguir dois tipos de regularização.\n",
    "\n",
    "### Regularização $L_1$ (LASSO)\n",
    "\n",
    "Consiste em impor uma restrição à soma dos módulos dos parâmetros $\\beta$ (exceto o intercepto $\\beta_0$):\n",
    "\n",
    "$$\n",
    "\\min_\\beta D(\\beta) = \\sum_{i=1}^n (y_i - f_\\beta(x_i))^2\\\\\n",
    "\\textrm{sujeito à $\\sum_{j=1}^p |\\beta_j| < s$}\n",
    "$$\n",
    "\n",
    "para alguma escolha de $s > 0$. Pode-se mostrar que isto é equivalente a\n",
    "\n",
    "$$\n",
    "\\min_\\beta \\sum_{i=1}^n (y_i - f_\\beta(x_i))^2 + \\alpha \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "\n",
    "para algum $\\alpha(s) > 0$.\n",
    "\n",
    "O efeito do termo de \"penalização\" $\\alpha \\sum_{j=1}^p |\\beta_j|$ é forçar que os $\\beta$'s sejam menores. Note que não estamos mais otimizando o desvio $D(\\beta)$. Ou seja, os parâmetros encontrados não serão os parâmetros de quadrados mínimos.\n",
    "\n",
    "Com a presença do módulo no termo de penalização, esta função objetivo deixa de ser derivável. Felizmente, uma solução aproximada pode ser encontrada em tempo polinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x*1.0\n",
    "x_powers = np.array([np.power(x,i) for i in range(1,19)]).T\n",
    "#x_powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1214901542478906 [ 9.71850356e-01  3.54654299e-03 -3.10308690e-04  1.24805949e-05\n",
      "  8.49748252e-07  2.50299909e-08  1.39669694e-10 -4.19188245e-11\n",
      " -3.79674990e-12 -2.31130423e-13 -1.15256639e-14 -4.85706029e-16\n",
      " -1.63063259e-17 -2.93979028e-19  1.52643412e-20  2.36499079e-21\n",
      "  1.99991736e-22  1.40399427e-23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabricio/anaconda2/envs/py3.5/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lr1 = Lasso(alpha=0.1)\n",
    "lr1.fit(x_powers,y)\n",
    "print(lr1.intercept_,lr1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularização $L_2$ (Ridge)\n",
    "\n",
    "A regularização $L_2$ é semelhante à $L_1$, mas usa a norma-2 como base para a penalização:\n",
    "\n",
    "$$\n",
    "\\min_\\beta D(\\beta) = \\sum_{i=1}^n (y_i - f_\\beta(x_i))^2\\\\\n",
    "\\textrm{sujeito à $\\sum_{j=1}^p \\beta_j^2 < s$}\n",
    "$$\n",
    "\n",
    "para alguma escolha de $s > 0$. Pode-se mostrar que isto é equivalente a\n",
    "\n",
    "$$\n",
    "\\min_\\beta  \\sum_{i=1}^n (y_i - f_\\beta(x_i))^2 + \\alpha \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "para algum $\\alpha(s) > 0$.\n",
    "\n",
    "De novo, o efeito do termo de \"penalização\" $\\alpha \\sum_{j=1}^p \\beta_j^2$ é forçar que os $\\beta$'s sejam menores. Esta penalidade é maior que no LASSO para $|\\beta_j| > 1$, mas menores que naquele para $|\\beta_j| < 1$.\n",
    "\n",
    "Ao contrário do LASSO, a solução exata para o Ridge pode ser calculada em tempo polinomial. Esta solução é dada por\n",
    "\n",
    "$$\n",
    "\\beta = (X^\\top X + \\alpha \\mathbf{I})^{-1} X^\\top y.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.505986784549927 [ 1.36516031e+00 -4.83493572e-07  1.11678717e-04  7.91826901e-02\n",
      "  4.14803667e-01 -7.35158137e-01  5.06316510e-01 -1.99025341e-01\n",
      "  5.10051809e-02 -9.08752555e-03  1.16429368e-03 -1.08983126e-04\n",
      "  7.47148221e-06 -3.71220470e-07  1.30126700e-08 -3.05146107e-10\n",
      "  4.29533208e-12 -2.74382777e-14]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lr2 = Ridge(alpha=0.1)\n",
    "lr2.fit(x_powers,y)\n",
    "print(lr2.intercept_,lr2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material suplementar\n",
    "\n",
    "[Complete tutorial Ridge and Lasso Regression in Python](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
